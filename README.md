# DotGame (Dots and Boxes)

Creating a popular game we play on paper and creating agents to play it perfectly.

---

## ðŸ§  Agents Implemented

Agents are modular and located in `agents/`:
- **Tabular Q-Learning** (`tabular_q.py`)
- **Deep Q-Network (DQN)** (`dqn.py`)
- **Minimax** (`minmax.py`)
- **Monte Carlo Tree Search (MCTS)** (`mcts.py`)
- **AlphaZero-style Agent** (`alphazero.py`)  
  Uses MCTS guided by policy/value neural networks.


---


## Directory Structure

DotGame/<br/>
â”œâ”€â”€ agents/<br/># All agent implementations<br/>
â”œâ”€â”€ env/<br/># DotGame environment<br/>
â”œâ”€â”€ notebook/<br/># Colabs file<br/>
â”œâ”€â”€ legacy/<br/># Old C implementation<br/>
â”œâ”€â”€ trained_models/<br># Saved model files<br/>
â”œâ”€â”€ plots/<br/># Plots saved here after play<br/>
â”œâ”€â”€ train.py<br/># Main training entry point<br/>
â”œâ”€â”€ play.py<br/># Main evaluation entry point<br/>
â””â”€â”€ requirements.txt<br/># Libraries used<br/> 
â””â”€â”€ README.md<br/># This file<br/>


---


## Usage

### Install dependencies
```bash
pip install -r requirements.txt
```

### Train an agent 

```bash
python3 -m train --dots 3 --agent mcts --epochs 400000
```

Supported agents: qt, dqn, mcts, alphazero

### Play bw agents

```bash
python3 -m play --dots 4 --agent1 self --agent2 alphazero --games 1
```

self = user input<br/>
win-loss rate would automatically be plotted after games at plots/


---


Legacy: legacy/dotgame.c

```bash
gcc dotgame.c
./a.out
```

![](dotgame.png)
