{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUGCd7IQVKPI",
        "outputId": "fb35534d-5dab-4b53-a3e3-2971aecf5ccc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "odDHsetaVSxT",
        "outputId": "af9ca383-c14c-4786-fc68-17953df29fa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nobj = Env(4)\\nturn = -1\\nwhile(not obj.gameover()):\\n  if turn ==-1:\\n    move = input(\"enter move\")\\n    reward = obj.step(int(move),turn)\\n    obj.render()\\n  elif turn  == 1:\\n    move = obj.minmax(turn)\\n    reward = obj.step(move,turn)\\n    obj.render()\\n  if reward == 0:\\n    turn = -turn\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "class Env:\n",
        "    dots = 0\n",
        "    grid = [0]\n",
        "    boxes = [0]\n",
        "    def clear_screen(self):\n",
        "        import os\n",
        "        os.system('cls' if os.name == 'nt' else 'clear')\n",
        "\n",
        "    def __init__(self,dots):\n",
        "        self.dots = dots\n",
        "        self.boxes = [0]*(dots-1)*(dots-1)\n",
        "        self.grid = [0]*2 * dots *(dots - 1)\n",
        "\n",
        "    def step(self,action,turn):\n",
        "        self.grid[action] = 1\n",
        "        rows,x = 0,0\n",
        "        reward = 0.0\n",
        "        rows = (action)//(2*self.dots-1)\n",
        "        x = (action)%(2*self.dots-1)\n",
        "        #print(rows,x,action,turn)\n",
        "        if(x >= 0 and x<= self.dots-2):\t#move is horizontal !\n",
        "            # no. of  row == col\n",
        "            #possible boxes index = [(col-1) or (col) ]* (gridsize-1) + x;\n",
        "            #for col-1 : check move, (row-1)(2*gridsize - 1) + x, move - gridsize, move - gridsize + 1\n",
        "            #for col : check move, (row+1)(2*gridsize -1)+x, move + gridsize, move+gridsize -1\n",
        "            if(rows !=0): #for col - 1\n",
        "                if(not (self.grid[action]==0 or self.grid[(rows-1)*(2*self.dots-1)+x]==0 or self.grid[action-self.dots]==0 or self.grid[action-self.dots+1]==0)):\n",
        "                    index = (rows-1)*(self.dots -1) + x\n",
        "                    #print(index)\n",
        "                    self.boxes[index] = turn\n",
        "                    reward = turn\n",
        "            if(rows != self.dots -1): # for col\n",
        "                if(not(self.grid[action]==0 or self.grid[(rows+1)*(2*self.dots-1)+x]==0 or self.grid[action+self.dots]==0 or self.grid[action+self.dots-1]==0)):\n",
        "                    #print(rows)\n",
        "                    index = rows*(self.dots-1) +x\n",
        "                    self.boxes[index] = turn\n",
        "                    reward = turn\n",
        "        else:   #move is vertical ~\n",
        "\t\t#rows = col+1\n",
        "            rows += 1\n",
        "            x = action - (rows*(self.dots-1) + (rows-1)*(self.dots))\n",
        "            #possible boxes index = (row-1) *(gridsize -1) + (x or x-1)\n",
        "            #for x-1 : check move, move-1, move - gridsize , move + gridsize - 1\n",
        "            #for x : check move,move+1, move - (gridsize -1), move + gridsize\n",
        "\n",
        "            if(x != 0): #for x-1\n",
        "                if(not(self.grid[action]==0 or self.grid[action-1]==0 or self.grid[action-(self.dots)]==0 or self.grid[action+self.dots-1]==0)):\n",
        "                    index = (rows-1)*(self.dots-1)+x-1\n",
        "                    self.boxes[index] = turn\n",
        "                    reward = turn\n",
        "            if(x != self.dots -1): #for x\n",
        "                #print(x)\n",
        "                if(not(self.grid[action]==0 or self.grid[action+1]==0 or self.grid[action-self.dots+1]==0 or self.grid[action+self.dots]==0)):\n",
        "                    index = (rows-1)*(self.dots-1)+x\n",
        "                    self.boxes[index] = turn\n",
        "                    reward = turn\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def render(self):\n",
        "        #self.clear_screen() #clear screen\n",
        "        boxindex = 0\n",
        "        gridindex = 0\n",
        "\n",
        "        for i in range(self.dots*self.dots):\n",
        "            print(\"\\033[42m  \\033[m\",end='')\n",
        "            if((i+1)%self.dots == 0):\n",
        "                print('\\n',end='')\n",
        "                if(i == self.dots*self.dots -1):\n",
        "                    break\n",
        "                for j in range(self.dots):\n",
        "                    if(self.grid[gridindex] == 1):\n",
        "                        print(\"\\033[47m  \\033[m\",end='')\n",
        "                    else:\n",
        "                        print(f\"\\033[40m{gridindex:2d}\\033[m\",end='')\n",
        "                    gridindex += 1\n",
        "                    #print(gridindex,end='')\n",
        "                    if(j<self.dots-1):\n",
        "                        #print(boxindex)\n",
        "                        box = self.boxes[boxindex]\n",
        "                        boxindex += 1\n",
        "                        if(box == 1):\n",
        "                            print(\"\\033[41m  \\033[m\",end='')\n",
        "                        elif(box == -1):\n",
        "                            print(\"\\033[44m  \\033[m\",end='')\n",
        "                        else:\n",
        "                            print(\"\\033[100m  \\033[m\",end='')\n",
        "                print(\"\\n\",end='')\n",
        "                continue\n",
        "            if(self.grid[gridindex] == 1):\n",
        "                print(\"\\033[47m  \\033[m\",end='')\n",
        "            else:\n",
        "                print(f\"\\033[40m{gridindex:2d}\\033[m\",end='')\n",
        "            gridindex += 1\n",
        "        print('\\n')\n",
        "\n",
        "    def gameover(self):\n",
        "        for i in range(len(self.grid)):\n",
        "            if(self.grid[i]==0):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def reset(self):\n",
        "        self.grid = [0]*(2*self.dots*(self.dots-1))\n",
        "        self.boxes = [0]*(self.dots-1)*(self.dots-1)\n",
        "        return self.grid.copy()\n",
        "\n",
        "    def action_space(self):\n",
        "      actions = []\n",
        "      for i in range(len(self.grid)):\n",
        "        if(self.grid[i] == 0):\n",
        "          actions.append(i)\n",
        "      return actions\n",
        "\n",
        "    @classmethod\n",
        "    def from_state(cls,dots, state):\n",
        "        env = cls(dots)\n",
        "        g_len = len(env.grid)\n",
        "        b_len = len(env.boxes)\n",
        "        env.grid = list(state[:g_len])\n",
        "        env.boxes = list(state[g_len:g_len + b_len])\n",
        "        env.turn = state[-1]\n",
        "        return env\n",
        "    @classmethod\n",
        "    def Gameover(cls,dots, state):\n",
        "        env = cls(dots)\n",
        "        g_len = len(env.grid)\n",
        "        b_len = len(env.boxes)\n",
        "        env.grid = list(state[:g_len])\n",
        "        env.boxes = list(state[g_len:g_len + b_len])\n",
        "        env.turn = state[-1]\n",
        "        return env.gameover()\n",
        "\n",
        "    def clone(self):\n",
        "        env = Env(self.dots)\n",
        "        for i in range(len(self.grid)):\n",
        "            env.grid[i] = self.grid[i]\n",
        "            if (i < len(self.boxes)):\n",
        "                env.boxes[i] = self.boxes[i]\n",
        "        return env\n",
        "\n",
        "#testing environment\n",
        "'''\n",
        "obj = Env(4)\n",
        "turn = -1\n",
        "while(not obj.gameover()):\n",
        "  if turn ==-1:\n",
        "    move = input(\"enter move\")\n",
        "    reward = obj.step(int(move),turn)\n",
        "    obj.render()\n",
        "  elif turn  == 1:\n",
        "    move = obj.minmax(turn)\n",
        "    reward = obj.step(move,turn)\n",
        "    obj.render()\n",
        "  if reward == 0:\n",
        "    turn = -turn\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bsfO-BzSH00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDrQU_EDVwWu"
      },
      "outputs": [],
      "source": [
        "#testing environment\n",
        "obj = Env(4)\n",
        "turn = 1\n",
        "while(not obj.gameover):\n",
        "  if turn ==1:\n",
        "    move = input(\"enter move\")\n",
        "    reward = obj.step(int(move),turn)\n",
        "    obj.render()\n",
        "  elif turn  == -1:\n",
        "    move = obj.minmax(turn)\n",
        "    reward = obj.step(move,turn)\n",
        "    obj.render()\n",
        "  if reward == 0:\n",
        "    turn = -turn\n",
        "\n",
        "del(obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIvqEYfa4MFF",
        "outputId": "54060920-1845-4b94-f10c-596b8e92508f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tabular Q-Learning\n"
          ]
        }
      ],
      "source": [
        "print(\"Tabular Q-Learning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "L1aLuAfVLZd6",
        "outputId": "4e5155fb-8c9a-4dac-c210-12713272b64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.0 0\n",
            "-1.0 50000\n",
            "1.0 100000\n",
            "-3.0 150000\n",
            "3.0 200000\n",
            "5.0 250000\n",
            "5.0 300000\n",
            "5.0 350000\n",
            "-7.0 400000\n",
            "9.0 450000\n",
            "3.0 500000\n",
            "3.0 550000\n",
            "3.0 600000\n",
            "9.0 650000\n",
            "5.0 700000\n",
            "1.0 750000\n",
            "9.0 800000\n",
            "1.0 850000\n",
            "-1.0 900000\n",
            "3.0 950000\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3493912259.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Q_dict.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n",
        "from env.env import Env\n",
        "from agents.minmax import Play as greed\n",
        "class qt:\n",
        "    def train(self,dots,epochs):\n",
        "        env = Env(dots)\n",
        "        Q = defaultdict(lambda: np.zeros(len(env.grid)))\n",
        "        discount_factor = 0.7\n",
        "        learning_rate = 0.1\n",
        "        #epochs  = 1000000\n",
        "        for episode in range(epochs+1):\n",
        "            turn = random.choice([1,-1])\n",
        "            state = tuple(env.grid + env.boxes + [turn])\n",
        "            while not env.gameover():\n",
        "                valid_actions = env.action_space()\n",
        "                epsilon = max(0.01,0.995**(episode))  # Exploration rate\n",
        "                q_state_action = 0\n",
        "                if random.random() < epsilon:\n",
        "                    action = random.choice(valid_actions) # Explore\n",
        "                else:\n",
        "                    if turn == 1:\n",
        "                        valid_q_values = [Q[state][a] for a in valid_actions]\n",
        "                        action = valid_actions[np.argmax(valid_q_values)] # Exploit\n",
        "                        q_state_action = Q[state][action]\n",
        "                    else:\n",
        "                        q_values = Q[state].copy()\n",
        "                        valid_q_values = [Q[state][a] for a in valid_actions]\n",
        "                        action = valid_actions[np.argmin(valid_q_values)]\n",
        "                        q_state_action = q_values[action]\n",
        "\n",
        "                reward= float(env.step(action,turn))\n",
        "                if(reward == 0):\n",
        "                    turn = -turn\n",
        "                if(env.gameover()):\n",
        "                    reward = sum(env.boxes)\n",
        "                    best_q_next = reward\n",
        "                else:\n",
        "                    next_state = tuple(env.grid + env.boxes+[turn])\n",
        "                    valid_actions_next = env.action_space()\n",
        "                    best_q_next = 0\n",
        "                    if turn == 1:\n",
        "                        valid_q_values = [Q[next_state][a] for a in valid_actions_next]\n",
        "                        best_q_next = np.max(valid_q_values) # Exploit\n",
        "                    else:\n",
        "                        valid_q_values = [Q[next_state][a] for a in valid_actions_next]\n",
        "                        best_q_next = np.min(valid_q_values) # Exploit\n",
        "\n",
        "                Q[state][action] += learning_rate * (reward + discount_factor * best_q_next - q_state_action)\n",
        "                #print(best_q_next)\n",
        "\n",
        "            env.reset()\n",
        "            if(episode%1000 == 0):\n",
        "                print(f\"no of q values stored: {len(Q)} ,episodes : {episode}\")\n",
        "\n",
        "        with open(f\"trained_models/Q{dots}.pkl\", \"wb\") as f:\n",
        "            pickle.dump(dict(Q),f)\n",
        "        #print(Q)\n",
        "        print(f\"training finished, model stored in trained_models/Q{dots}.pkl\")\n",
        "\n",
        "class Play:\n",
        "   def play(self,env,turn,secs=0):\n",
        "        dots = env.dots\n",
        "        Q = defaultdict(lambda: np.zeros(len(env.grid)))\n",
        "        try:\n",
        "            with open(f\"trained_models/Q{dots}.pkl\",\"r\") as f:\n",
        "                Q = defaultdict(lambda: np.zeros(len(env.grid)), pickle.load(f))\n",
        "        except Exception:\n",
        "            #print(\"Please train the model first, playing randomnly now\")\n",
        "            greedy = greed()\n",
        "            return greedy.play(env,turn)\n",
        "        state = tuple(env.grid + env.boxes + [turn])\n",
        "        valid_actions = env.action_space()\n",
        "        valid_q_values = [Q[state][a] for a in valid_actions]\n",
        "        action = valid_actions[np.argmax(valid_q_values)]\n",
        "        return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ObZ8lC33GD85",
        "outputId": "2a35e5db-ae66-4d8f-f448-794a29413c5f"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_71cd8ad3-af38-4c08-a1ba-2cd511a444c3\", \"Q.pkl\", 18269041)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pickle\n",
        "with open('Q.pkl', 'wb') as f:\n",
        "    pickle.dump(dict(Q),f)\n",
        "from google.colab import files\n",
        "files.download('Q.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ss2JWc0gGzRc",
        "outputId": "acb7c98a-26e9-42ec-8d50-c4558680f0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/MyDrive/Q.pkl', 'wb') as f:\n",
        "    pickle.dump(dict(Q), f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b9hm07CBCSD",
        "outputId": "a6f1d61d-733d-4df4-cb39-c0ea3a4e5cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NOW WORKING ON DEEP Q-Learning\n"
          ]
        }
      ],
      "source": [
        "print(\"NOW WORKING ON DEEP Q-Learning\")\n",
        "\n",
        "# so what i actually do is start an episode,get state and choose epsilon greedy action from q(state), run step and get next_state, reward to compute\n",
        "# Q(s,a) = Q(s,a) + learning_rate * (reward + discount_factor * max(Q(s')) - Q(s,a))\n",
        "# and move on to next state until episode ends and keep on doing it until epochs\n",
        "\n",
        "# but here its a neural network\n",
        "# in neural network we have loss functions and adam optimizer\n",
        "# basically my update is slowly updating my q(s,a) towards actual reward by moving it slowly towards reward+discount*q'(s',a') using gradient descent\n",
        "# so my loss function in tabular q-learning is (reward+discount*q'(s',a')-q(s,a)), what should it be in nn? maybe square of it? but why not just linear\n",
        "# and i think I can pass in my loss function to adam optimizer and run optimizer after every step just like in tabular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDihYi_IHOD0"
      },
      "outputs": [],
      "source": [
        "##initializing neural network\n",
        "import torch.nn as nn\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "VVzpD9DZMUTg",
        "outputId": "49df7d7e-ae85-4e64-f65d-ab00ad3a7a7a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-363530987.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mfused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__dynamo_disable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisable_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mdisable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchdynamo_logging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_rules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m from .bytecode_analysis import (\n\u001b[1;32m     32\u001b[0m     \u001b[0mget_indexof\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/trace_rules.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mresume_execution\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTORCH_DYNAMO_RESUME_IN_PREFIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNP_SUPPORTED_MODULES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwrap_if_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from .variables import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mFunctionalCallVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mUntypedStorageVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m )\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchCtxManagerClassVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTorchInGraphFunctionVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m from .user_defined import (\n\u001b[1;32m    111\u001b[0m     \u001b[0mMutableMappingVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/variables/torch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fx_tracing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_onnx_export\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "##initializing neural network\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from env.env import Env\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "##training deepQ with self play\n",
        "\n",
        "class dqn:\n",
        "  def train(self,dots,epochs):\n",
        "    discount_factor = 0.65\n",
        "    env = Env(dots)\n",
        "    state = tuple(env.grid + env.boxes+[1])\n",
        "    input_dim = len(state)\n",
        "    output_dim = len(env.grid)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = NN(input_dim, output_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for episode in range(epochs):\n",
        "      turn = 1\n",
        "      reward = 0\n",
        "      best_q_next = 0\n",
        "      q_state_action = 0\n",
        "      while(not env.gameover()):\n",
        "        state = tuple(env.grid + env.boxes+[turn])\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        q_state = model.forward(x=state_tensor).squeeze()\n",
        "        valid_actions = env.action_space()\n",
        "        epsilon = max(0.01,0.1*(0.995**(episode)))  # Exploration rate\n",
        "        q_state_action = 0\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(valid_actions) # Explore\n",
        "            q_state_action = q_state[action]\n",
        "        else: #Exploit\n",
        "          valid_q_values = torch.tensor([q_state[a] for a in valid_actions],device=device)\n",
        "          if turn == 1:\n",
        "            action = valid_actions[torch.argmax(valid_q_values)] # Exploit\n",
        "            q_state_action = q_state[action]\n",
        "          else:\n",
        "            action = valid_actions[torch.argmin(valid_q_values)] # Exploit\n",
        "            q_state_action = q_state[action]\n",
        "\n",
        "        reward= float(env.step(action,turn))/len(env.boxes)\n",
        "        if(reward == 0):\n",
        "          turn = -turn\n",
        "        if(env.gameover()):\n",
        "           reward = sum(env.boxes)\n",
        "           best_q_next = reward\n",
        "        else :\n",
        "          next_state = tuple(env.grid + env.boxes+[turn])\n",
        "          next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "          q_next_state = model.forward(x=next_state_tensor).squeeze()\n",
        "          valid_actions_next = env.action_space()\n",
        "          best_q_next = 0\n",
        "          if valid_actions_next:\n",
        "            valid_q_values = torch.tensor([q_next_state[a] for a in valid_actions_next],device=device)\n",
        "            if turn == 1:\n",
        "              best_q_next = torch.max(valid_q_values)\n",
        "            else:\n",
        "              best_q_next = torch.min(valid_q_values)\n",
        "        loss = (reward + discount_factor * best_q_next - q_state_action)**2\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      env.reset()\n",
        "      if(episode%10000 == 0):\n",
        "        print(\"{episode} epochs done\")\n",
        "    print(\"training done\")\n",
        "    torch.save(model.state_dict(), f\"dqn{dots}.pth\")\n",
        "    print(f\"model saved at trained_models/dqn{dots}.pth\")\n",
        "\n",
        "\n",
        "class Play:\n",
        "   def play(self,env,turn,secs=0):\n",
        "      dots = env.dots\n",
        "      state = tuple(env.grid + env.boxes + [turn])\n",
        "      input_dim = len(state)\n",
        "      output_dim = len(env.grid)\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      model = NN(input_dim, output_dim).to(torch.device(device))\n",
        "      try:\n",
        "        model.load_state_dict(torch.load(f\"trained_models/dqn{dots}.pth\", map_location=torch.device(device)))\n",
        "      except FileNotFoundError:\n",
        "          print(\"Please train the model first, playing randomnly now\")\n",
        "      state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "      with torch.no_grad():\n",
        "        q_values = model(state_tensor).squeeze()\n",
        "        valid_actions = env.action_space()\n",
        "        valid_q_values = torch.tensor([q_values[a] for a in valid_actions],device=device)\n",
        "        if turn == 1:\n",
        "          action = valid_actions[torch.argmax(valid_q_values).item()]\n",
        "        else:\n",
        "          action = valid_actions[torch.argmin(valid_q_values).item()]\n",
        "\n",
        "      return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ttYWstR1lXPK",
        "outputId": "f79a6d16-97ff-4098-f83e-a2fd88e155b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_64a97476-418d-4040-b555-0d34354246d3\", \"dqn_dotgame.pth\", 98608)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Save\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/dqn_dotgame.pth')\n",
        "# Load\n",
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/dqn_dotgame.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnEZ3oUrzVIP",
        "outputId": "42df1bc6-fc32-4434-91b4-2723b09d4b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "504154 495846 0\n",
            "896683 103317 0\n"
          ]
        }
      ],
      "source": [
        "##so both my Qtable and DeepQ have been trained\n",
        "# now letting them play against each other and plotting results\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "device = 'cpu'\n",
        "env = Env(4)\n",
        "state = tuple(env.grid + env.boxes)\n",
        "input_dim = len(state)\n",
        "output_dim = len(env.grid)\n",
        "model = DQN(input_dim, output_dim).to(torch.device('cpu'))\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/dqn_dotgame.pth'))\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/dqn_dotgame.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Q.pkl\", \"rb\") as f:\n",
        "    Q = defaultdict(lambda: np.zeros(len(env.grid)), pickle.load(f))\n",
        "\n",
        "model.eval()\n",
        "wins_Qdeep1=0\n",
        "wins_Qtable=0\n",
        "draw_deep1=0\n",
        "N= 1000000\n",
        "#let deep q play as player 1 first\n",
        "for games in range(N):\n",
        "  env.reset()\n",
        "  turn = 1\n",
        "  total_reward = 0\n",
        "  while(not env.gameover()):\n",
        "    state = tuple(env.grid + env.boxes)\n",
        "    if turn == 1:\n",
        "      state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "      with torch.no_grad():\n",
        "        q_values = model(state_tensor).squeeze()\n",
        "        #print(q_values)\n",
        "        valid_actions = env.action_space()\n",
        "        valid_q_values = torch.tensor([q_values[a] for a in valid_actions],device=device)\n",
        "        action = valid_actions[torch.argmax(valid_q_values).item()]\n",
        "        #print(action)\n",
        "    else:\n",
        "      valid_actions = env.action_space()\n",
        "      action = random.choice(valid_actions)\n",
        "      #valid_q_values = [Q[state][a] for a in valid_actions]\n",
        "      #action = valid_actions[np.argmin(valid_q_values)]\n",
        "\n",
        "    reward = float(env.step(action,turn))\n",
        "    #env.render()\n",
        "    total_reward += reward\n",
        "    if(reward == 0):\n",
        "      turn = -turn\n",
        "  if(total_reward>0):\n",
        "    wins_Qdeep1 += 1\n",
        "  elif(total_reward<0):\n",
        "    wins_Qtable += 1\n",
        "  else:\n",
        "    draw_deep1 += 1\n",
        "\n",
        "# tableQ plays as player 1\n",
        "wins_Qdeep = 0\n",
        "wins_Qtable1 = 0\n",
        "draw_table1 = 0\n",
        "for games in range(N):\n",
        "  env.reset()\n",
        "  turn = 1\n",
        "  total_reward = 0\n",
        "  while(not env.gameover()):\n",
        "    state = tuple(env.grid + env.boxes)\n",
        "    if turn == -1:\n",
        "      valid_actions = env.action_space()\n",
        "      action = random.choice(valid_actions)\n",
        "      ''' state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "      with torch.no_grad():\n",
        "        q_values = model(state_tensor).squeeze()\n",
        "        print(q_values)\n",
        "        valid_actions = env.action_space()\n",
        "        valid_q_values = torch.tensor([q_values[a] for a in valid_actions],device=device)\n",
        "        action = valid_actions[torch.argmin(valid_q_values).item()]\n",
        "        print(action) '''\n",
        "    else:\n",
        "      q_values = Q[state]\n",
        "      valid_actions = env.action_space()\n",
        "      valid_q_values = [Q[state][a] for a in valid_actions]\n",
        "      action = valid_actions[np.argmax(valid_q_values)]\n",
        "\n",
        "    reward = float(env.step(action,turn))\n",
        "    total_reward += reward\n",
        "    if(reward == 0):\n",
        "      turn = -turn\n",
        "  if(total_reward>0):\n",
        "    wins_Qdeep += 1\n",
        "  elif(total_reward<0):\n",
        "    wins_Qtable1 += 1\n",
        "  else:\n",
        "    draw_table1 += 1\n",
        "\n",
        "print(wins_Qdeep1,wins_Qtable,draw_deep1)\n",
        "print(wins_Qdeep,wins_Qtable1,draw_table1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "t8lN30g7416s",
        "outputId": "61ed661a-a23f-443e-c8ed-19e89c98c150"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# bar plot to compare models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Stats\n",
        "labels = [\"DeepQ\", \"TabularQ\"]\n",
        "\n",
        "wins = [wins_Qdeep1, wins_Qtable1]\n",
        "losses = [wins_Qtable, wins_Qdeep]\n",
        "draws = [draw_deep1, draw_table1]\n",
        "\n",
        "x = np.arange(len(labels))  # label locations\n",
        "width = 0.25  # width of each bar\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width, wins, width, label='Wins', color='green')\n",
        "rects2 = ax.bar(x, draws, width, label='Draws', color='gray')\n",
        "rects3 = ax.bar(x + width, losses, width, label='Losses', color='red')\n",
        "\n",
        "# Labeling\n",
        "ax.set_ylabel('Number of Games')\n",
        "ax.set_title('DeepQ, TabularQ vs Random (Win/Draw/Loss)')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "# Show numbers on top of bars (optional)\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{int(height)}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#monte carlo"
      ],
      "metadata": {
        "id": "amK9oL41DWew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56NYQcKFQtvv"
      },
      "outputs": [],
      "source": [
        "#now monte carlo tree search\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "class Node:\n",
        "  def __init__(self,env,state,turn,parent=None,action=None,position=None):\n",
        "    self.env = env\n",
        "    self.state = state\n",
        "    self.turn = turn\n",
        "    self.position = position\n",
        "    self.parent = parent\n",
        "    self.action = action\n",
        "    self.children = []\n",
        "\n",
        "    self.total_reward = 0\n",
        "    self.visits = 0\n",
        "\n",
        "\n",
        "  def is_expanded(self):\n",
        "    return (len(self.children) == len(self.env.action_space()))\n",
        "\n",
        "  def expand(self):\n",
        "    for action in self.env.action_space():\n",
        "      new_env = self.env.copy()\n",
        "      reward = new_env.step(action,self.turn)\n",
        "      if reward == 0:\n",
        "        new_turn = -self.turn\n",
        "      else:\n",
        "        new_turn = self.turn\n",
        "      new_state = tuple(new_env.grid + new_env.boxes + [new_turn])\n",
        "      if(new_state in self.position):\n",
        "        new_node = self.position[new_state]\n",
        "      else:\n",
        "        new_node = Node(new_env,new_state,new_turn,self,action,self.position)\n",
        "        self.position[new_state] = new_node\n",
        "        self.children.append(new_node)\n",
        "\n",
        "  def best_child(self,c_param=1.4):\n",
        "    values = [\n",
        "            (child.total_reward / (child.visits + 1e-8)) +\n",
        "            c_param * math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-8))\n",
        "            for child in self.children\n",
        "        ]\n",
        "    if self.turn == 1:\n",
        "      return self.children[values.index(max(values))]\n",
        "    else:\n",
        "      return self.children[values.index(min(values))]\n",
        "\n",
        "class MCTS:\n",
        "  def __init__(self,root,env,turn):\n",
        "    state = tuple(env.grid + env.boxes + [turn])\n",
        "    self.position = {}\n",
        "    self.root = Node(env,state,turn,self.position)\n",
        "    self.turn = turn\n",
        "    self.env = env\n",
        "\n",
        "\n",
        "  def think(self,secs):\n",
        "    start_time = time.time()\n",
        "    end_time = time.time()\n",
        "    while(end_time - start_time < secs):\n",
        "      node = self.root\n",
        "      turn = node.turn\n",
        "      env = node.env.copy()\n",
        "      reward = node.total_reward\n",
        "      #what i needed to do is either find the best node or expand node to simulate and get rewards for backpropagation\n",
        "      if(not node.env.gameover()):\n",
        "        while(node.children and node.is_expanded()):\n",
        "          node = node.best_child()\n",
        "\n",
        "        if(not node.children):\n",
        "          node.expand()\n",
        "          node = random.choice(node.children)\n",
        "        reward = self.simulate(node)\n",
        "\n",
        "      self.backpropagate(node,reward)\n",
        "      end_time = time.time()\n",
        "\n",
        "  def train(self,epochs):\n",
        "    for i in range(epochs):\n",
        "      node = self.root\n",
        "      turn = node.turn\n",
        "      env = node.env.copy()\n",
        "      reward = node.total_reward\n",
        "      #what i needed to do is either find the best node or expand node to simulate and get rewards for backpropagation\n",
        "      if(not node.env.gameover()):\n",
        "        while(node.children and node.is_expanded()):\n",
        "          node = node.best_child()\n",
        "\n",
        "        if(not node.children):\n",
        "          node.expand()\n",
        "          node = random.choice(node.children)\n",
        "        reward = self.simulate(node)\n",
        "      self.backpropagate(node,reward)\n",
        "\n",
        "  def simulate(self,node):\n",
        "    env = node.env.copy()\n",
        "    turn = node.turn\n",
        "    total_reward = 0\n",
        "    while(not env.gameover()):\n",
        "      action = random.choice(env.action_space())\n",
        "      reward = float(env.step(action,turn))\n",
        "      if(reward == 0):\n",
        "        turn = -turn\n",
        "      total_reward+=reward\n",
        "    return total_reward\n",
        "\n",
        "  def backpropagate(self,node,reward):\n",
        "    while(node):\n",
        "      node.total_reward += reward\n",
        "      node.visits += 1\n",
        "      reward = -reward\n",
        "      node = node.parent\n",
        "\n",
        "  def computer_move(self,secs):\n",
        "    self.think(secs)\n",
        "    best_node = self.root.best_child()\n",
        "    self.root = best_node\n",
        "    return best_node.action\n",
        "\n",
        "env = Env(dots=4)\n",
        "mcts = MCTS(env,1)\n",
        "mcts.train(epochs=10)\n",
        "with open(\"mcts.pkl\", \"wb\") as f:\n",
        "    pickle.dump(mcts, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NcEjB9NwqMS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class NN:\n",
        "  def __init__(self,input_dim=24+9+1,output_dim=24+1):\n",
        "    super().__init():\n",
        "      self.model = nn.Sequential(\n",
        "          nn.Linear(input_dim, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 128),\n",
        "          nn.ReLU(),\n",
        "      )\n",
        "    self.policy_head = nn.Linear(128, output_dim)\n",
        "    self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "  def predict(self,input):\n",
        "    network = self.model(input)\n",
        "    return self.policy_head(network),self.value_head(network)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now alphazero"
      ],
      "metadata": {
        "id": "MKh53nceDQNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KS2iyv3VQHAW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "#from env.env import Env\n",
        "class NN(nn.Module):  # Inherit properly\n",
        "  def __init__(self, input_dim,output_dim):\n",
        "        super(NN, self).__init__()  # Correct super call\n",
        "        self.model = nn.Sequential(\n",
        "        nn.Linear(input_dim, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128,128),\n",
        "        nn.ReLU()\n",
        "        )\n",
        "        self.policy_head = nn.Linear(128, output_dim)\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    network = self.model(x)\n",
        "    return self.policy_head(network),self.value_head(network)\n",
        "\n",
        "class Node:\n",
        "  def __init__(self,dots,state,parent=None,action=None):\n",
        "    self.dots = dots\n",
        "    self.state = state\n",
        "    self.parent = parent\n",
        "    self.action = action\n",
        "    self.children = {}\n",
        "    self.N = {}\n",
        "    self.total_visits = 0\n",
        "    self.W = {}\n",
        "\n",
        "\n",
        "  def is_expanded(self):\n",
        "    return (len(self.children) == len(Env.from_state(self.dots,self.state).action_space()))\n",
        "\n",
        "  def expand(self):\n",
        "    turn = self.state[-1]\n",
        "    env = Env.from_state(self.dots,self.state)\n",
        "    for action in env.action_space():\n",
        "      new_env = env.clone()\n",
        "      reward = new_env.step(action,turn)\n",
        "      new_turn = 0\n",
        "      if reward == 0:\n",
        "        new_turn = -turn\n",
        "      else :\n",
        "        new_turn = turn\n",
        "      self.W[action] = 0\n",
        "      self.N[action] = 0\n",
        "      state = tuple(new_env.grid + new_env.boxes +[new_turn])\n",
        "      child = Node(self.dots,state,parent=self,action=action)\n",
        "      self.children[action]=child\n",
        "\n",
        "  def best_child(self,model,c_param=1.2,training=True):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    policy,_ = model(torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0))\n",
        "    policy = policy.squeeze(0)\n",
        "    policy = policy.tolist()\n",
        "    #print(policy)\n",
        "    #policy = policy.detach()\n",
        "    grid = 2 *self.dots*(self.dots-1)\n",
        "    if training == True:\n",
        "      alpha = 0.3\n",
        "      epsilon = 0.25\n",
        "      noise = np.random.dirichlet([alpha] * grid)\n",
        "      for i in range(grid):\n",
        "        policy[i] = (1 - epsilon) * policy[i] + epsilon * noise[i]\n",
        "\n",
        "    utility = [0]*(grid)\n",
        "    for action in range(grid):\n",
        "      if action not in self.N.keys():\n",
        "        utility[action] = self.state[-1]*0.005\n",
        "      else:\n",
        "        Q_s_a = self.W[action]/(self.N[action]+1e-8)\n",
        "        U_s_a = c_param*policy[action]*math.sqrt(self.total_visits)/(1+self.N[action])\n",
        "        utility[action] = Q_s_a + U_s_a\n",
        "    valid_actions = Env.from_state(self.dots,self.state).action_space()\n",
        "    best = random.choice(valid_actions)\n",
        "    #print(utility)\n",
        "    turn = self.state[-1]\n",
        "    if turn == 1:\n",
        "      max = utility[best]\n",
        "      for action in valid_actions:\n",
        "        if utility[action] > max:\n",
        "          max = utility[action]\n",
        "          best = action\n",
        "    elif turn == -1:\n",
        "      min = utility[best]\n",
        "      for action in valid_actions:\n",
        "        if utility[action] < min:\n",
        "          min = utility[action]\n",
        "          best = action\n",
        "    #self.N[best] += 1\n",
        "    #print(len(self.children))\n",
        "    #print(valid_actions)\n",
        "    #print(best)\n",
        "    return best\n",
        "\n",
        "\n",
        "\n",
        "class alphazero:\n",
        "  def __init__(self):\n",
        "    self.replay_buffer = deque(maxlen=100000)\n",
        "\n",
        "\n",
        "  def backpropagate(self,node,reward,t=0.5,discount=1):\n",
        "    while node:\n",
        "      if node.parent:\n",
        "        #print(f\"current state {node.state} parent action : {node.action} parent rewards : {node.parent.W} parent visits : {node.parent.W}\")\n",
        "        node.parent.W[node.action] += reward\n",
        "        node.parent.N[node.action] += 1\n",
        "      policy = [0]*2*node.dots*(node.dots-1)\n",
        "      node.total_visits += 1\n",
        "      reward *=discount\n",
        "      valid_actions = Env.from_state(node.dots,node.state).action_space()\n",
        "      for action in valid_actions:\n",
        "        z = node.total_visits\n",
        "        if z == 0 :\n",
        "          policy[action] = 0 #aggressively explore unexplored actions\n",
        "        else:\n",
        "          policy[action] = node.N[action]**(1/t) / (z**(1/t))\n",
        "      self.replay_buffer.append((node.state, policy, reward))\n",
        "      if not node.parent:\n",
        "        break\n",
        "\n",
        "      node = node.parent\n",
        "    return node\n",
        "\n",
        "  def show(self, node, depth=0):\n",
        "      indent = \"  \" * depth\n",
        "      print(f\"{indent}visits: {node.N}, total reward: {node.W}\")\n",
        "\n",
        "      for action, child in node.children.items():\n",
        "          #expected_reward = child.total_reward / (child.visits + 1e-8)\n",
        "          print(f\"{indent} Action {action}  visits: {node.N}, total reward: {node.W}\")\n",
        "          self.show(child, depth + 1)\n",
        "\n",
        "      if not node.children:\n",
        "          print(f\"{indent} [Leaf node]\")\n",
        "\n",
        "  def mcts(self,dots,root,simulations,model,training=True):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    node = root\n",
        "    #print(f\"recieved this {node.state}\")\n",
        "    #self.show(node)\n",
        "    threshold = 2*dots*(dots-1)\n",
        "    movecount = 0\n",
        "    for _ in range(simulations):\n",
        "      #print(f\"simulation {_}\")\n",
        "      #print(f\"current tree \")\n",
        "      #self.show(node)\n",
        "      # Selection\n",
        "      t = 1 if movecount < 5 else 0.1\n",
        "      while node.children and node.is_expanded():\n",
        "          min_visits = 2*dots*(dots-1)\n",
        "          min_action = -9\n",
        "          for action in list(node.children.keys()):\n",
        "            if node.children[action].total_visits < min_visits:\n",
        "              min_visits = node.children[action].total_visits\n",
        "              min_action = action\n",
        "          if min_visits < threshold:\n",
        "            action = min_action\n",
        "          else:\n",
        "            action = node.best_child(model,training)\n",
        "          #print(f\"choosing best child {action}\")\n",
        "          node = node.children[action]\n",
        "          movecount += 1\n",
        "          if Env.Gameover(dots,node.state):\n",
        "              movecount = 0\n",
        "              break\n",
        "\n",
        "      # Terminal node\n",
        "      if Env.Gameover(dots,node.state):\n",
        "          reward = sum(Env.from_state(dots,node.state).boxes)\n",
        "          value = 1 if reward > 0 else -1 if reward < 0 else 0\n",
        "          #print(f\"backpropagating this {node.state} {value}\")\n",
        "          node = self.backpropagate(node, value,t)\n",
        "          movecount = 0\n",
        "          #print(f\"got this {node.state}\")\n",
        "          continue\n",
        "\n",
        "      # Expansion\n",
        "      node.expand()\n",
        "      #print(\"expanded node\")\n",
        "      #print(f\"node visits {node.N} node w {node.W}\")\n",
        "      # Evaluation\n",
        "      state_tensor = torch.tensor(node.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "      _, predicted_value = model(state_tensor)\n",
        "      value = predicted_value.item()\n",
        "      #print(f\"backpropagating this {node.state} {value}\")\n",
        "      node = self.backpropagate(node, value,t)\n",
        "\n",
        "      #print(f\"got this {node.state}\")\n",
        "    #print(f\"returning this {node.state}\")\n",
        "    #self.show(node)\n",
        "    return node\n",
        "\n",
        "  def train(self,dots,epochs=2,n=1000):\n",
        "    env = Env(dots)\n",
        "    state = tuple(env.grid + env.boxes+[1])\n",
        "    input_dim = len(state)\n",
        "    output_dim = len(env.grid)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = NN(input_dim,output_dim).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "    checkpoint = torch.load(\"alphazero4_checkpoint_epoch_2000.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    for episode in range(epochs):\n",
        "      #print(episode)\n",
        "      if episode < 1000:\n",
        "        lr = 0.1\n",
        "      elif episode < 2000:\n",
        "        lr = 0.02\n",
        "      elif episode < 3000:\n",
        "        lr = 0.002\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "      if episode % 500 == 0 and not episode == 0:\n",
        "        print(f\"{episode} episodes done, saving checkpoint in trained_models/\")\n",
        "        torch.save({\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "          }, f\"alphazero{dots}_checkpoint_epoch_{episode}.pt\")\n",
        "        from google.colab import files\n",
        "        files.download(f'alphazero{dots}_checkpoint_epoch_{episode}.pt')\n",
        "\n",
        "      #play n games to collect training data for nn\n",
        "      root = Node(dots,state)\n",
        "      self.mcts(dots=dots,root=root,simulations=n,model=model)\n",
        "      while(len(self.replay_buffer) < 10000):\n",
        "          self.mcts(dots=dots,root=root,simulations=n,model=model)\n",
        "        #print(\"simulations done\")\n",
        "      #take random sample games for training\n",
        "      batch = random.sample(self.replay_buffer, k=10000)\n",
        "      X = torch.tensor([s for (s, _, _) in batch], dtype=torch.float32, device=device)\n",
        "      Y_policy = torch.tensor([p for (_, p, _) in batch], dtype=torch.float32, device=device)\n",
        "      Y_value = torch.tensor([v for (_, _, v) in batch], dtype=torch.float32, device=device)\n",
        "      model.train()\n",
        "      pred_policies, pred_values = model(X)\n",
        "      log_policies = torch.log_softmax(pred_policies, dim=1)   # [B, 24]\n",
        "      policy_loss = nn.KLDivLoss(reduction=\"batchmean\")(log_policies, Y_policy)\n",
        "      value_loss = nn.MSELoss()(pred_values.squeeze(), Y_value)\n",
        "      total_loss = policy_loss + value_loss\n",
        "      #print(f\"policy requires grad: {pred_policies.requires_grad}\")\n",
        "      #print(f\"value requires grad: {pred_values.requires_grad}\")\n",
        "      for name, param in model.named_parameters():\n",
        "        if param.grad is not None and param.grad.isnan().any():\n",
        "          print(f\"NaN in gradients: {name}\")\n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "      if (episode %100 == 0):\n",
        "        policy_probs_batch = torch.softmax(pred_policies, dim=1)\n",
        "        print(f\"Episode {episode}: Loss = {total_loss.item():.4f}\")\n",
        "        entropy = - (policy_probs_batch * (policy_probs_batch+1e-8).log()).sum(dim=1).mean().item()\n",
        "        print(f\"Policy entropy: {entropy:.4f}\")\n",
        "    print(\"training done\")\n",
        "    torch.save({\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "          }, f\"alphazero{dots}.pt\")\n",
        "    #saving optimizer for continuous training\n",
        "    print(f\"Model saved at alphazero{dots}.pt\")\n",
        "\n",
        "\n",
        "class Play:\n",
        "  def play(self,env,turn,sims=5000):\n",
        "    dots = env.dots\n",
        "    state = tuple(env.grid + env.boxes+[1])\n",
        "    input_dim = len(state)\n",
        "    output_dim = len(env.grid)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = NN(input_dim,output_dim).to(device)\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    try:\n",
        "      checkpoint = torch.load(f\"alphazero{dots}.pt\", map_location=torch.device(device))\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    except FileNotFoundError:\n",
        "      print(\"Please train the model first, playing randomnly now\")\n",
        "\n",
        "    state = tuple(env.grid + env.boxes + [turn])\n",
        "    livenode = Node(dots,state)\n",
        "    bot = alphazero()\n",
        "    livenode = bot.mcts(dots,livenode,sims,model,False)\n",
        "    for i in livenode.N.keys():\n",
        "      print(f\"action {i} total reward: {livenode.W[i]/(livenode.N[i]+0.1)}\")\n",
        "    action = livenode.best_child(model,False)\n",
        "    print(f\"choosing action {action}\")\n",
        "    #bot.show(livenode)\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SF3DcdCbGDk"
      },
      "outputs": [],
      "source": [
        "class Minmax:\n",
        "    def play(self,env,turn,secs=0):\n",
        "        import random\n",
        "        ok_actions = []\n",
        "        for action in env.action_space():\n",
        "            new_env = env.clone()\n",
        "            reward = new_env.step(action,turn)\n",
        "            if reward * turn > 0:\n",
        "                return action\n",
        "            elif reward == 0:\n",
        "                ok_actions.append(action)\n",
        "        draws = []\n",
        "        for action in ok_actions:\n",
        "            new_env = env.clone()\n",
        "            new_env.step(action,turn)\n",
        "            draw = True\n",
        "            for action1 in new_env.action_space():\n",
        "                new_env1 = new_env.clone()\n",
        "                reward = new_env1.step(action1,-turn)\n",
        "                if reward * (-turn) == 1:\n",
        "                    draw = False\n",
        "                    break\n",
        "            if draw:\n",
        "                draws.append(action)\n",
        "        #print(draws)\n",
        "        if draws:\n",
        "            return random.choice(draws)\n",
        "        else:\n",
        "            return random.choice(env.action_space())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY5BPmhEPakD"
      },
      "outputs": [],
      "source": [
        "# bar plot to compare models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class Plot:\n",
        "    def plot(self,player1,player2,wins,draws,loss,dots):\n",
        "\n",
        "        #labels = [player1]\n",
        "        #x = np.arange(len(labels))  # label locations\n",
        "        x = 1\n",
        "        width = 0.1  # width of each bar\n",
        "        #games = wins+loss+draws\n",
        "        # Plot\n",
        "        fig, ax = plt.subplots()\n",
        "        rects1 = ax.bar(x - width, wins, width, label=f'{player1} Wins', color='green')\n",
        "        rects2 = ax.bar(x, draws, width, label='Draws', color='gray')\n",
        "        rects3 = ax.bar(x + width, loss, width, label=f'{player2}  wins', color='blue')\n",
        "\n",
        "        # Labeling\n",
        "        ax.set_ylabel('Number of Games')\n",
        "        ax.set_title(f'{dots}x{dots} {player1} vs {player2}')\n",
        "        ax.set_xlabel('')\n",
        "        ax.legend()\n",
        "\n",
        "        # Show numbers on top of bars (optional)\n",
        "        def autolabel(rects):\n",
        "            for rect in rects:\n",
        "                height = rect.get_height()\n",
        "                ax.annotate(f'{int(height)}',\n",
        "                            xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                            xytext=(0, 3), textcoords=\"offset points\",\n",
        "                            ha='center', va='bottom')\n",
        "\n",
        "        autolabel(rects1)\n",
        "        autolabel(rects2)\n",
        "        autolabel(rects3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{dots}x{dots}{player1}vs{player2}.png\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xjMBuLFMEhRk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "ed4678eb-9de2-4220-fc59-8e873b218309"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Minmax' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3432332644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphazero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Minmax' is not defined"
          ]
        }
      ],
      "source": [
        "bot = alphazero()\n",
        "greedy = Minmax()\n",
        "bot.train(dots=3,epochs=5000,n=2000)\n",
        "player1 = Play()\n",
        "env = Env(3)\n",
        "turn = 1\n",
        "win = 0\n",
        "loss = 0\n",
        "draw = 0\n",
        "for _ in range(10):\n",
        "  if(_ % 100 == 0):\n",
        "    print(f\"{_} games done\")\n",
        "  while not env.gameover():\n",
        "    if turn == 1:\n",
        "      action = player1.play(env,turn)\n",
        "    else:\n",
        "      action = greedy.play(env,turn)\n",
        "    v = env.step(action,turn)\n",
        "    env.render()\n",
        "    if v == 0:\n",
        "      turn = -turn\n",
        "  if sum(env.boxes) > 0: win+=1\n",
        "  elif sum(env.boxes) <0: loss+=1\n",
        "  else: draw += 1\n",
        "  env.reset()\n",
        "  turn = 1\n",
        "plotter = Plot()\n",
        "print(f\"alphazero {win} minmax {loss} draw {draw}\")\n",
        "plotter.plot(\"alphazero\",\"minmax\",win,draw,loss,3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot = alphazero()\n",
        "bot.train(4,epochs=3000,n=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gKGRQSurj1k",
        "outputId": "79f6ed7e-d389-4d76-df60-393a3009bccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Loss = 22.6624\n",
            "Policy entropy: 0.2001\n",
            "Episode 100: Loss = 21.0758\n",
            "Policy entropy: 0.0372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aTrc9ZYj7YFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}